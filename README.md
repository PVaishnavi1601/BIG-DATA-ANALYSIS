# BIG-DATA-ANALYSIS
Perform analysis on a large dataset using tools like PYSPARK or DASK to demonstrate scalability

**COMPANY:** CODTECH IT SOLUTIONS

**MAME:** P VAISHNAVI

**INTERN ID:** CT08DH1432

**DOMAIN:** Data Analytics

**DURATION:** 8 WEEKS

**MENTOR:** NEELA SANTOSH

Task Description:
For the first task of my internship, I performed large-scale data analysis using PySpark to demonstrate the scalability of big data processing. I worked with the New York City Yellow Taxi trip dataset, which contained millions of records. My goal was to clean, transform, and analyze the dataset to extract actionable insights, such as peak travel hours, popular passenger counts, and average fares.

To complete the task, I made extensive use of YouTube tutorials for practical demonstrations of PySpark operations, environment setup, and optimization techniques. These visual explanations helped me understand how Spark sessions work, how to efficiently load and clean large datasets, and how to apply transformations like filtering, grouping, and aggregations. Additionally, I referred to quickref.me for concise PySpark syntax references, function usage, and examples. This quick-access documentation allowed me to quickly recall command structures and avoid common syntax errors during implementation.

By following this approach, I learned how to handle large datasets that would otherwise be too slow to process using standard Python libraries like Pandas. I also gained hands-on experience in using Spark SQL functions, working with Parquet files, and writing analysis results to CSV for reporting. The task improved my ability to combine online learning resources effectively — YouTube for conceptual clarity and quickref.me for rapid syntax lookup — enabling me to complete the analysis efficiently. Overall, this experience enhanced both my technical skills in distributed data processing and my problem-solving abilities when working with real-world big data projects.

Output:

